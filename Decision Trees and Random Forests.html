<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Trees and Random Forests</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Aptos, -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background-color: #e7feff;
            overflow-x: hidden;
        }

        .slide {
            width: 960px;
            height: 540px;
            margin: 20px auto;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: flex-start;
            padding: 64px;
            background-color: #e7feff;
            color: #1d2951;
            flex-shrink: 0;
            position: relative;
        }

        .slide.center {
            justify-content: center;
            align-items: center;
            text-align: center;
        }

        h1 {
            font-size: 4rem;
            font-weight: bold;
            margin-bottom: 1rem;
            color: #1d2951;
        }

        h2 {
            font-size: 2.5rem;
            font-weight: bold;
            margin-bottom: 1.5rem;
            color: #1d2951;
        }

        h3 {
            font-size: 1.75rem;
            font-weight: bold;
            margin-bottom: 1rem;
            color: #1d2951;
        }

        p {
            font-size: 1.25rem;
            line-height: 1.6;
            margin-bottom: 0.75rem;
            color: #1d2951;
        }

        ul, ol {
            margin-left: 2rem;
            margin-top: 0.5rem;
        }

        li {
            font-size: 1.25rem;
            line-height: 1.8;
            margin-bottom: 0.75rem;
            color: #1d2951;
        }

        .code-box {
            background-color: #367588;
            color: #fdf5e6;
            padding: 1.5rem;
            border-radius: 8px;
            margin-top: 1rem;
            font-family: 'Courier New', monospace;
            font-size: 0.95rem;
            overflow-x: auto;
            line-height: 1.5;
        }

        .code-box strong {
            color: #fdf5e6;
        }

        .code-box .keyword {
            color: #ff7f50;
        }

        .code-box .function {
            color: #87ceeb;
        }

        .code-box .string {
            color: #90ee90;
        }

        .code-box .comment {
            color: #b0c4de;
            font-style: italic;
        }

        .code-box .number {
            color: #ffd700;
        }

        .two-column {
            display: flex;
            gap: 2rem;
            width: 100%;
            align-items: flex-start;
        }

        .column {
            flex: 1;
        }

        .highlight-box {
            background-color: #367588;
            color: #fdf5e6;
            padding: 1.5rem;
            border-radius: 8px;
            margin-top: 1rem;
            border-left: 5px solid #fdf5e6;
        }

        .highlight-box h3 {
            color: #fdf5e6;
            margin-top: 0;
        }

        .highlight-box p, .highlight-box li {
            color: #fdf5e6;
        }

        .subtitle {
            font-size: 1.75rem;
            color: #1d2951;
            margin-top: 0.5rem;
        }

        .metadata {
            font-size: 1.1rem;
            color: #1d2951;
            margin-top: 2rem;
        }

        .tooltip-container {
            position: relative;
            display: inline-block;
            border-bottom: 2px dotted #367588;
            cursor: help;
        }

        .tooltip-container:hover .tooltip-text {
            visibility: visible;
            opacity: 1;
        }

        .tooltip-text {
            visibility: hidden;
            width: 280px;
            background-color: #367588;
            color: #fdf5e6;
            text-align: left;
            padding: 1rem;
            border-radius: 6px;
            position: absolute;
            z-index: 1000;
            bottom: 125%;
            left: 50%;
            margin-left: -140px;
            opacity: 0;
            transition: opacity 0.3s;
            font-size: 0.95rem;
            line-height: 1.5;
            border: 2px solid #fdf5e6;
        }

        .tooltip-text::after {
            content: "";
            position: absolute;
            top: 100%;
            left: 50%;
            margin-left: -5px;
            border-width: 5px;
            border-style: solid;
            border-color: #367588 transparent transparent transparent;
        }

        .slide-image {
            max-width: 90%;
            max-height: 350px;
            margin: 1rem auto;
            display: block;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15);
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 1rem;
        }

        th, td {
            border: 2px solid #1d2951;
            padding: 0.75rem;
            text-align: center;
            color: #1d2951;
            font-size: 1.05rem;
        }

        th {
            background-color: #367588;
            color: #fdf5e6;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: rgba(54, 117, 136, 0.15);
        }

        .link-list {
            background-color: #367588;
            color: #fdfd96;
            padding: 1.5rem;
            border-radius: 8px;
            margin-top: 0.5rem;
        }

        .link-list ol {
            margin-left: 1.5rem;
        }

        .link-list li {
            margin-bottom: 0.6rem;
            font-size: 1rem;
        }

        .feature-box {
            background-color: #367588;
            color: #fdf5e6;
            padding: 1.2rem;
            border-radius: 8px;
            margin-bottom: 1rem;
        }

        .feature-box h4 {
            color: #fdf5e6;
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }

        .feature-box p {
            color: #fdf5e6;
            font-size: 0.95rem;
            margin: 0;
        }
    </style>
</head>
<body>

    <!-- Slide 1: Title Slide -->
    <section class="slide center">
        <h1>Decision Trees and Random Forests</h1>
        <p class="subtitle">Machine Learning Classification Methods</p>
        <p class="subtitle">using scikit-learn and Heart Disease Dataset</p>
        <p class="metadata">Python code explanation</p>
    </section>

    <!-- Slide 2: What are Decision Trees? -->
    <section class="slide">
        <h2>What are Decision Trees?</h2>
        <ul>
            <li><strong>Decision Tree:</strong> A supervised learning algorithm for classification</li>
            <li><strong>Structure:</strong> Tree-like model with nodes representing decision points and branches representing outcomes</li>
            <li><strong>Root Node:</strong> Starting point containing the entire dataset</li>
            <li><strong>Internal Nodes:</strong> Decision points based on feature values</li>
            <li><strong>Leaf Nodes:</strong> Final predictions or class labels</li>
        </ul>
        <div class="highlight-box">
            <h3>Key Advantage</h3>
            <p>Highly interpretable and visualizable - easy to understand how decisions are made</p>
        </div>
    </section>

    <!-- Slide 3: How Decision Trees Work -->
    <section class="slide">
        <h2>How Decision Trees Work</h2>
        <ol>
            <li><strong>Root Node Selection:</strong> Choose the feature that maximizes information gain or minimizes impurity (using Gini Index or Entropy)</li>
            <li><strong>Recursive Splitting:</strong> Split dataset into subsets based on feature values</li>
            <li><strong>Branching:</strong> Each branch represents a different outcome path</li>
            <li><strong>Stopping Criteria:</strong> Stop when nodes are pure, max depth reached, or subset too small</li>
            <li><strong>Prediction:</strong> New data follows branches down to reach a leaf node prediction</li>
        </ol>
    </section>

    <!-- Slide 4: What are Random Forests? -->
    <section class="slide">
        <h2>What are Random Forests?</h2>
        <ul>
            <li><strong>Random Forest:</strong> Ensemble method combining multiple decision trees</li>
            <li><strong>Bootstrap Sampling:</strong> Random rows selected with replacement to train each tree</li>
            <li><strong>Feature Randomness:</strong> Each tree uses random subset of features</li>
            <li><strong>Majority Voting:</strong> Final prediction is the most common prediction among all trees</li>
            <li><strong>Advantage:</strong> Reduces overfitting and improves accuracy compared to single decision tree</li>
        </ul>
    </section>

    <!-- Slide 5: Key Differences -->
    <section class="slide">
        <h2>Decision Trees vs Random Forests</h2>
        <table>
            <tr>
                <th>Feature</th>
                <th>Decision Tree</th>
                <th>Random Forest</th>
            </tr>
            <tr>
                <td>Structure</td>
                <td>Single tree</td>
                <td>Multiple trees (ensemble)</td>
            </tr>
            <tr>
                <td>Overfitting</td>
                <td>Prone to overfitting</td>
                <td>Resistant to overfitting</td>
            </tr>
            <tr>
                <td>Interpretability</td>
                <td>Highly interpretable</td>
                <td>Less interpretable</td>
            </tr>
            <tr>
                <td>Accuracy</td>
                <td>Moderate</td>
                <td>Generally higher</td>
            </tr>
        </table>
    </section>

    <!-- Slide 6: Heart Disease Dataset Overview -->
    <section class="slide">
        <h2>Heart Disease Dataset Overview</h2>
        <ul>
            <li><strong>Source:</strong> UCI Machine Learning Repository</li>
            <li><strong>Instances:</strong> 303 patient records</li>
            <li><strong>Target:</strong> Presence of heart disease (0 = absence, 1-4 = presence levels)</li>
            <li><strong>Total Attributes:</strong> 14 key features</li>
        </ul>
        <div class="highlight-box">
            <h3>Dataset Purpose</h3>
            <p>Binary classification: Predict whether a patient has heart disease based on medical measurements and tests</p>
        </div>
    </section>

    <!-- Slide 7: Numerical Features in Heart Disease Dataset -->
    <section class="slide">
        <h2>Numerical Features (5 variables)</h2>
        <div class="feature-box">
            <h4>age</h4>
            <p>Patient's age in years</p>
        </div>
        <div class="feature-box">
            <h4>trestbps</h4>
            <p>Resting blood pressure in mm Hg</p>
        </div>
        <div class="feature-box">
            <h4>chol</h4>
            <p>Serum cholesterol level in mg/dl</p>
        </div>
        <div class="feature-box">
            <h4>thalach</h4>
            <p>Maximum heart rate achieved during exercise</p>
        </div>
        <div class="feature-box">
            <h4>oldpeak</h4>
            <p>ST depression induced by exercise relative to rest</p>
        </div>
    </section>

    <!-- Slide 8: Categorical Features in Heart Disease Dataset -->
    <section class="slide">
        <h2>Categorical Features (8 variables)</h2>
        <div class="feature-box">
            <h4>sex</h4>
            <p>Patient's sex (categorical: 1=male, 0=female)</p>
        </div>
        <div class="feature-box">
            <h4>cp</h4>
            <p>Chest pain type (categorical: 1-4, typical angina to asymptomatic)</p>
        </div>
        <div class="feature-box">
            <h4>fbs, restecg, exang, slope, ca, thal</h4>
            <p>Various diagnostic tests and measurements (categorical encodings)</p>
        </div>
    </section>

    <!-- Slide 9: Data Preprocessing Steps -->
    <section class="slide">
        <h2 style="margin-bottom: 0.8rem;">Data Preprocessing in Script</h2>
        <div style="font-size: 0.85rem; margin-top: 0.5rem; overflow-y: auto; max-height: 430px;">
            <p style="margin-bottom: 0.5rem;"><strong>1. Load and Explore Data:</strong></p>
            <div class="code-box" style="font-size: 0.8rem; padding: 0.8rem; margin-top: 0.3rem; margin-bottom: 0.3rem;">
                filepath = r"C:\Users\ksenia\Desktop\PythonProject\Heart.csv"<br>
                raw_df = pd.read_csv(filepath)<br>
                raw_df.dropna(subset=['target'], inplace=True)
            </div>
            <p style="margin-bottom: 0.5rem; margin-top: 0.5rem;"><strong>2. Split Dataset (60% Train, 20% Val, 20% Test):</strong></p>
            <div class="code-box" style="font-size: 0.8rem; padding: 0.8rem; margin-top: 0.3rem; margin-bottom: 0.3rem;">
                train_df, temp_df = train_test_split(raw_df, test_size=<span class="number">0.4</span>)<br>
                val_df, test_df = train_test_split(temp_df, test_size=<span class="number">0.5</span>)
            </div>
            <p style="margin-bottom: 0.5rem; margin-top: 0.5rem;"><strong>3. Define Features and Target:</strong></p>
            <div class="code-box" style="font-size: 0.8rem; padding: 0.8rem; margin-top: 0.3rem; margin-bottom: 0.3rem;">
                input_cols = list(train_df.columns)[<span class="number">0:-1</span>]<br>
                train_inputs = train_df[input_cols].copy()<br>
                train_targets = train_df[<span class="string">'target'</span>].copy()
            </div>
            <p style="margin-bottom: 0.5rem; margin-top: 0.5rem;"><strong>4. Identify Feature Types:</strong></p>
            <div class="code-box" style="font-size: 0.75rem; padding: 0.8rem; margin-top: 0.3rem;">
                categorical_features = [<span class="string">'sex'</span>, <span class="string">'cp'</span>, <span class="string">'fbs'</span>, <span class="string">'restecg'</span>, <span class="string">'exang'</span>, <span class="string">'slope'</span>, <span class="string">'ca'</span>, <span class="string">'thal'</span>]<br>
                numerical_features = [<span class="string">'age'</span>, <span class="string">'trestbps'</span>, <span class="string">'chol'</span>, <span class="string">'thalach'</span>, <span class="string">'oldpeak'</span>]
            </div>
        </div>
    </section>

    <!-- Slide 10: Feature Scaling: MinMaxScaler -->
    <section class="slide">
        <h2>Feature Scaling: MinMaxScaler</h2>
        <div class="code-box">
            <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> <span class="tooltip-container">MinMaxScaler<span class="tooltip-text"><strong>MinMaxScaler:</strong> Scales numerical features to [0,1] range using min-max normalization. Prevents features with larger ranges from dominating the model.</span></span></span><br><br>
            scaler = MinMaxScaler()<br>
            train_inputs[numerical_features] = scaler.<span class="tooltip-container"><span class="function">fit_transform</span><span class="tooltip-text"><strong>fit_transform:</strong> Learns min/max values from training data and applies normalization. Essential step in preprocessing pipeline.</span></span></span>(train_inputs[numerical_features])
        </div>
        <div class="highlight-box">
            <h3>Why Scale Numerical Features?</h3>
            <p>Ensures all features have similar ranges (0-1), preventing features with larger ranges from dominating the model</p>
        </div>
    </section>

    <!-- Slide 11: Categorical Encoding: OneHotEncoder -->
    <section class="slide">
        <h2>Categorical Encoding: OneHotEncoder</h2>
        <div class="code-box">
            <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> <span class="tooltip-container">OneHotEncoder<span class="tooltip-text"><strong>OneHotEncoder:</strong> Converts categorical variables into binary columns. For example, 'sex' becomes sex_0 and sex_1. Essential for tree-based models.</span></span></span><br><br>
            encoder = <span class="tooltip-container">OneHotEncoder<span class="tooltip-text"><strong>OneHotEncoder params:</strong> drop='first' avoids multicollinearity, sparse_output=False returns dense array</span></span></span>(drop=<span class="string">'first'</span>, sparse_output=<span class="keyword">False</span>)<br>
            encoded = encoder.<span class="tooltip-container"><span class="function">fit_transform</span><span class="tooltip-text"><strong>fit_transform:</strong> Learns unique categories and transforms data to binary format in one step.</span></span></span>(train_inputs[categorical_features])
        </div>
        <div class="highlight-box">
            <h3>Why One-Hot Encoding?</h3>
            <p>Converts categorical variables (sex, cp, etc.) into binary columns (0 or 1) that decision trees and random forests can process</p>
        </div>
    </section>

    <!-- Slide 12: Training Decision Tree Classifier -->
    <section class="slide">
        <h2>Training Decision Tree Classifier</h2>
        <div class="code-box">
            <span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> <span class="tooltip-container">DecisionTreeClassifier<span class="tooltip-text"><strong>DecisionTreeClassifier:</strong> Builds a decision tree by recursively splitting on features that best separate the target classes using Gini impurity or entropy.</span></span></span><br><br>
            model = <span class="tooltip-container">DecisionTreeClassifier<span class="tooltip-text"><strong>Parameters:</strong> max_depth=3 limits tree depth, random_state=42 ensures reproducibility</span></span></span>(max_depth=<span class="number">3</span>, random_state=<span class="number">42</span>)<br>
            model.<span class="tooltip-container"><span class="function">fit</span><span class="tooltip-text"><strong>fit:</strong> Trains the decision tree model on the training data using the specified hyperparameters.</span></span></span>(train_inputs, train_targets)<br><br>
            <span class="comment"># Make predictions</span><br>
            predictions = model.<span class="tooltip-container"><span class="function">predict</span><span class="tooltip-text"><strong>predict:</strong> Uses trained tree to classify new samples by following decision paths from root to leaf nodes.</span></span></span>(train_inputs)
        </div>
    </section>

    <!-- Slide 13: First Decision Tree Visualization -->
    <section class="slide">
        <h2>First Decision Tree Structure (max_depth=3)</h2>
        <img src="https://agi-prod-file-upload-public-main-use1.s3.amazonaws.com/dd9b7a1a-d8af-4c96-b158-d5218ee414e6" alt="Decision Tree with max_depth=3" class="slide-image">
        <div class="highlight-box" style="margin-top: 1rem; padding: 1rem; font-size: 1.1rem;">
            <p><strong>Root Node:</strong> Starting point with split condition on thal_2 feature</p>
            <p><strong>Branches:</strong> True and False paths representing feature value comparisons</p>
            <p><strong>Internal Nodes:</strong> Decision points showing Gini impurity and samples</p>
            <p><strong>Leaf Nodes (dark boxes):</strong> Final predictions with class labels</p>
        </div>
    </section>

    <!-- Slide 14: Training Random Forest Classifier -->
    <section class="slide">
        <h2>Training Random Forest Classifier</h2>
        <div class="code-box">
            <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier<br><br>
            model = RandomForestClassifier(n_estimators=<span class="number">100</span>, max_depth=<span class="number">10</span>,<br>
            &nbsp;&nbsp;min_samples_split=<span class="number">2</span>, n_jobs=<span class="number">-1</span>)<br>
            model.<span class="function">fit</span>(train_inputs, train_targets)<br><br>
            <span class="comment"># Make predictions</span><br>
            predictions = model.<span class="function">predict</span>(val_inputs)<br>
            accuracy = accuracy_score(val_targets, predictions)
        </div>
        <div class="highlight-box">
            <h3>Key Parameters</h3>
            <p>n_estimators: Number of trees (100 trees balance accuracy and training time)</p>
        </div>
    </section>

    <!-- Slide 14: Model Evaluation and Hyperparameter Tuning -->
    <section class="slide">
        <h2>GridSearchCV for Hyperparameter Tuning</h2>
        <div class="code-box">
            <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV<br><br>
            param_grid = {<br>
            &nbsp;&nbsp;<span class="string">'n_estimators'</span>: [<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>],<br>
            &nbsp;&nbsp;<span class="string">'max_depth'</span>: [<span class="keyword">None</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>],<br>
            &nbsp;&nbsp;<span class="string">'max_features'</span>: [<span class="string">'sqrt'</span>, <span class="string">'log2'</span>]}<br><br>
            grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=<span class="number">5</span>)
        </div>
    </section>

    <!-- Slide 15: Performance Results -->
    <section class="slide">
        <h2>Model Performance Results</h2>
        <table>
            <tr>
                <th>Model</th>
                <th>Training Accuracy</th>
                <th>Validation Accuracy</th>
                <th>Status</th>
            </tr>
            <tr>
                <td>Decision Tree (max_depth=3)</td>
                <td>83.25%</td>
                <td>82.93%</td>
                <td>Balanced</td>
            </tr>
            <tr>
                <td>Random Forest (default)</td>
                <td>93.17%</td>
                <td>~93%</td>
                <td>Better</td>
            </tr>
            <tr>
                <td>Random Forest (tuned)</td>
                <td>~95%</td>
                <td>95.12%</td>
                <td>Best</td>
            </tr>
        </table>
        <p style="margin-top: 1.5rem;"><strong>Observation:</strong> Random Forests achieve higher accuracy and better generalization compared to single Decision Tree</p>
    </section>

    <!-- Slide 16: Understanding Tree Depth and Metrics -->
    <section class="slide" style="justify-content: flex-start; padding-top: 40px; padding-bottom: 40px;">
        <h2 style="margin-bottom: 1rem;">Understanding Tree Depth and Error Metrics</h2>
        <div class="highlight-box" style="margin-top: 0.8rem; padding: 1.2rem;">
            <p style="margin-bottom: 0.6rem;"><strong>Depth:</strong> Number of levels from root to leaf node. Root is depth 0.</p>
            <p style="margin-bottom: 0.6rem;"><strong>Max Depth:</strong> Maximum allowed depth parameter. Controls tree complexity and prevents overfitting.</p>
            <p style="margin-bottom: 0.6rem;"><strong>Accuracy Score:</strong> Percentage of correct predictions.</p>
            <p style="margin-bottom: 0.6rem;"><strong>Error Rate:</strong> Percentage of incorrect predictions. Calculated as: 1 - Accuracy Score</p>
            <p style="margin-top: 0.8rem;"><span class="code-box" style="margin-top: 0.3rem; padding: 0.6rem; font-size: 0.9rem;">accuracy = accuracy_score(train_targets, predictions)</span></p>
        </div>
    </section>

    <!-- Slide 18: Overfitting vs Underfitting Analysis -->
    <section class="slide" style="justify-content: flex-start; padding-top: 25px; padding-bottom: 25px;">
        <h2 style="margin-bottom: 0.6rem; font-size: 2.2rem;">Overfitting vs Underfitting</h2>
        <img src="https://agi-prod-file-upload-public-main-use1.s3.amazonaws.com/9bea1bcb-ff0a-4813-b40f-8f41fd680ae1" alt="Max Depth vs Error Rate" class="slide-image" style="max-height: 240px; margin-bottom: 0.6rem;">
        <div class="highlight-box" style="margin-top: 0.4rem; padding: 0.8rem; font-size: 0.95rem;">
            <p style="margin-bottom: 0.3rem;"><strong>Underfitting (Depth 1-2):</strong> Both errors high. Model too simple.</p>
            <p style="margin-bottom: 0.3rem;"><strong>Optimal Fit (Depth 3):</strong> Errors balanced and minimal. Best generalization.</p>
            <p style="margin-bottom: 0.3rem;"><strong>Overfitting (Depth 8+):</strong> Training error low but validation error increases.</p>
            <p style="margin-bottom: 0;"><strong>Best max_depth:</strong> Where validation error is minimized (depth = 3)</p>
        </div>
    </section>

    <!-- Slide 19: Feature Importance Visualization -->
    <section class="slide" style="justify-content: flex-start; padding-top: 20px; padding-bottom: 20px;">
        <h2 style="margin-bottom: 0.4rem; font-size: 2rem;">Feature Importance in the Models</h2>
        <img src="https://agi-prod-file-upload-public-main-use1.s3.amazonaws.com/a6718dd9-43fd-4670-9a75-80d46e3a1764" alt="Feature Importances (Decision Tree)" class="slide-image" style="max-height: 220px; margin-bottom: 0.4rem;">
        <div class="highlight-box" style="margin-top: 0.3rem; padding: 0.7rem; font-size: 0.88rem;">
            <p style="margin-bottom: 0.2rem;"><strong>Top 5 Most Important Features:</strong></p>
            <p style="margin-bottom: 0.1rem;">1. thal_2 (Thalium test) - 25.6%</p>
            <p style="margin-bottom: 0.1rem;">2. thalach (Max heart rate) - 15.2%</p>
            <p style="margin-bottom: 0.1rem;">3. oldpeak (ST depression) - 12.8%</p>
            <p style="margin-bottom: 0.1rem;">4. age (Patient's age) - 12.2%</p>
            <p style="margin-bottom: 0.15rem;">5. chol (Cholesterol) - 11.7%</p>
            <p style="margin-bottom: 0; margin-top: 0.3rem; font-size: 0.82rem;"><strong>Use Case:</strong> Feature importance helps identify the most critical factors for heart disease diagnosis</p>
        </div>
    </section>

    <!-- Slide 20: Useful Resources and Links -->
    <section class="slide" style="justify-content: flex-start; padding-top: 40px; padding-bottom: 40px;">
        <h2 style="margin-bottom: 1rem;">Useful Learning Resources</h2>
        <div class="link-list" style="font-size: 0.95rem; max-height: 380px; overflow-y: auto; color: #fff000; background-color: #367588;">
            <ol style="margin-bottom: 0; color: #fff000;">
                <li style="margin-bottom: 0.5rem; color: #fff000;"><strong style="color: #fff000;">Scikit-learn Official Documentation:</strong> <span style="color: #fff000;">https://scikit-learn.org/stable/modules/tree.html</span></li>
                <li style="margin-bottom: 0.5rem; color: #fff000;"><strong style="color: #fff000;">Random Forest Classifier Guide:</strong> <span style="color: #fff000;">https://scikit-learn.org/stable/modules/ensemble.html</span></li>
                <li style="margin-bottom: 0.5rem; color: #fff000;"><strong style="color: #fff000;">UCI Machine Learning Repository:</strong> <span style="color: #fff000;">https://archive.ics.uci.edu/dataset/45/heart+disease</span></li>
                <li style="margin-bottom: 0.5rem; color: #fff000;"><strong style="color: #fff000;">Decision Tree Algorithm Explained:</strong> <span style="color: #fff000;">https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html</span></li>
                <li style="margin-bottom: 0.5rem; color: #fff000;"><strong style="color: #fff000;">Random Forest Algorithm Tutorial:</strong> <span style="color: #fff000;">https://www.datacareer.de/blog/random-forest-in-python-with-scikit-learn/</span></li>
                <li style="margin-bottom: 0.5rem; color: #fff000;"><strong style="color: #fff000;">Heart Disease Dataset (Kaggle):</strong> <span style="color: #fff000;">https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset/</span></li>
                <li style="margin-bottom: 0.5rem; color: #fff000;"><strong style="color: #fff000;">FreeCodeCamp Machine Learning Channel:</strong> <span style="color: #fff000;">https://www.youtube.com/c/freecodecamp</span></li>
                <li style="color: #fff000;"><strong style="color: #fff000;">Heart Disease Prediction Examples:</strong> <span style="color: #fff000;">https://github.com/sharmaroshan/Heart-UCI-Dataset</span></li>
            </ol>
        </div>
    </section>

</body>
</html>
